---
title: "KNN"
author: "Golnaz Abrashami, Tyler Norlund"
date: "2/24/19"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: FALSE
    number_sections: False
    fig_height: 3
    fig_width: 8
    fig.align: "center"
---

```{R libraries, message = FALSE, warning = FALSE}
library(pracma)
library(dplyr)
library(ggplot2)
library(latex2exp)
```

In this problem, you will implement a Distance-Weighted Nearest Neighbor Classifier and run it on a 2-dimensional dataset. (You canâ€™t use built-in knn functions in R to do this problem). You have to experiment with different distance measures and observe their influence on the classification performance.

The training data has two classes as shown in figure 1. The training set along with test points and their correct label are saved in knnData.csv.

# Exploratory Data Analysis

The data was given to us using CSV format. First, we read the data into respective dataframes.

```{R read_data}
dat <- read.csv("knnData.csv", header = TRUE)
tr <- dat %>% select(1,2,3)
ts <- dat %>% select(4,5,6)
rm(dat)
```

## Test Points

The first set of points we will look at are the testing data points. This will be what we compare our calculated data against. 

```{R test_viz}
ggplot(ts, aes(x=ts$testPoints_x1, y=ts$testPoints_x2, color=ts$testLabel)) + 
  geom_point(shape=1) + 
  theme(legend.position="none") + 
  labs(title= "Test Points", x = parse(text = TeX("x_{1}")), y = parse(text = TeX("x_{2}"))) +
  theme(plot.title = element_text(size=rel(1.8), color="#377BB5"),
        axis.title.x = element_text(size=rel(1.2)),
        axis.title.y = element_text(size=rel(1.2)))
```

Looing at the distribution, we can see that there _is_ a seperation of the datapoints, left and right. 

## Train Points

Now the training datapoints will be what we use to calculate the distances. 

```{R train_viz}
ggplot(ts, aes(x=tr$trainPoints_x1, y=tr$trainPoints_x2, color=tr$trainLabel)) + 
  geom_point(shape=16) + 
  theme(legend.position="none") + 
  labs(title= "Train Points", x = parse(text = TeX("x_{1}")), y = parse(text = TeX("x_{2}"))) +
  theme(plot.title = element_text(size=rel(1.8), color="#377BB5"),
        axis.title.x = element_text(size=rel(1.2)),
        axis.title.y = element_text(size=rel(1.2)), 
        legend.position ="none")
```

Looing at how the points are distributed, we can see that there _is_ a seperation of the datapoints, left and right. Let's see if we can get a good classification rate!

# Calculating Distance

So in order to calculate the weighted distance, we need to define some functions. First, the generic function used to calulate the weighted distances, 

$$\hat{f}(x_{q}) \leftarrow \frac{\sum_{i=1}^{k}w_{i}f(x_{i})}{\sum_{i=1}^{k}w_{i}} \text{,}$$

can be used to calculate the weighted distance for each test point, $x_{q}$. The weight, 

$$w_{i}=\frac{1}{d(x_{q}, x_{i})^{2}} \text{,}$$

is used to normalize the distances. The options we have are $L_{1}$, $L_{2}$, and $L_{\infty}$. The programatic function will need to have a condition for inifinity; infinity is not a number.

$$\begin{align}
L_{1}&: \sum |x_{i}| \\
L_{2}&: \sum x_{i}^{2} \\
L_{\infty}&: \texttt{max}\,( x_{i}) \\
\end{align}
$$

```{R weighted_distance}
weighted_distance <- function(x_indx, norm_indx) {
  if (norm_indx == 'inf') {
    d <- data.frame(
      x1 = abs((tr$trainPoints_x1 - ts$testPoints_x1[x_indx])),
      x2 = abs(tr$trainPoints_x2 - ts$testPoints_x2[x_indx]))
    d <- pmax(d$x1, d$x2)
  } else {
    d <- (abs((tr$trainPoints_x1 - ts$testPoints_x1[x_indx])^norm_indx) + 
      (abs(tr$trainPoints_x2 - ts$testPoints_x2[x_indx])^norm_indx))^1/norm_indx
  }
  d <- data.frame(distance = d, weight=1/(d^2), label=tr$trainLabel)
  return(d[order(d$distance),])
}
```

# KNN

With the weighted distance out of the way, we can use the function with the unique parameters. In this problem, we will apply a nearest neighbors solution, with $k=3$.

```{R knn}
knn <- function(x_indx, norm_indx, k) {
  n <- head(weighted_distance(x_indx, norm_indx), k)
  class <- which.max(
    array(data = c(sum(n[n$label == -1,]$weight), 
                   sum(n[n$label == 1,]$weight))
          )/sum(n$weight))
  if(sum(n[n$label == -1,]$weight)/sum(n$weight) > sum(n[n$label == 1,]$weight)/sum(n$weight)) {
    return(-1)
  } else {
    return(1)
  }
}
```

With the equations and the functions out of the way, we can apply 3-NN using the different distance measures.

```{R calculation}
l2 <- lapply(1:40, knn, 2, 3)
l1 <- lapply(1:40, knn, 1, 3)
linf <- lapply(1:40, knn, 'inf', 3)
```
# Accuracy

```{R accuracy_viz}
df <- data.frame(
  l=1:3,
  accuracy=c(sum(l2 == ts$testLabel)/40, sum(l1 == ts$testLabel)/40, sum(linf == ts$testLabel)/40))

ggplot(df, aes(x=l, y=accuracy)) +
  geom_bar(stat="identity", fill="#377BB5") +
  geom_text(aes(label=unlist(df$accuracy)), vjust=1.5, color="white", size=5) +
  labs(title= "3-NN Accuracy", x = "Laplacian", y = "Accuracy") +
  theme(plot.title= element_text(size=rel(1.8), color="#377BB5"),
        axis.title.x= element_text(size=rel(1.2)),
        axis.title.y= element_text(size=rel(1.2)),
        axis.text.x = element_text(size=rel(2))) +
  scale_x_discrete(limits = 1:3, labels= c(parse(text=TeX('$L_{2}$')),parse(text=TeX('$L_{1}$')),parse(text=TeX('$L_{\\infty}$'))))
  
```
