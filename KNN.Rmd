---
title: "KNN"
author: "Golnaz Abrashami, Tyler Norlund"
date: "2/24/19"
output: 
html_document:
toc: TRUE
toc_float:
collapsed: FALSE
number_sections: False
fig_height: 3
fig_width: 8
fig.align: "center"
---

```{R}
library(pracma)
library(dplyr)
library(ggplot2)
#library(e1071)
library(latex2exp)
```

In this problem, you will implement a Distance-Weighted Nearest Neighbor Classifier and run it on a 2-dimensional dataset. (You canâ€™t use built-in knn functions in R to do this problem). You have to experiment with different distance measures and observe their influence on the classification performance.

The training data has two classes as shown in figure 1. The training set along with test points and their correct label are saved in knnData.csv.

![figure 1](plot.png)

```{R}
dat <- read.csv("knnData.csv", header = TRUE)
tr <- dat %>% select(1,2,3)
ts <- dat %>% select(4,5,6)
rm(dat)
```

# Weighted Distance

So in order to calculate the weighted distance, we need to define some functions. First, the generic function used to calulate the weighted distances, 

$$\hat{f}(x_{q}) \leftarrow \frac{\sum_{i=1}^{k}w_{i}f(x_{i})}{\sum_{i=1}^{k}w_{i}} \text{,}$$

can be used to calculate the weighted distance for each test point, $x_{q}$. The weight, 

$$w_{i}=\frac{1}{d(x_{q}, x_{i})^{2}} \text{,}$$

is used to normalize the distances. The options we have are $L_{1}$, $L_{2}$, and $L_{\infty}$. The programatic function will need to have a condition for inifinity; infinity is not a number.

$$\begin{align}
L_{1}&: \sum |x_{i}| \\
L_{2}&: \sum x_{i}^{2} \\
L_{\infty}&: \texttt{max}\,( x_{i}) \\
\end{align}
$$

```{R}
weighted_distance <- function(x_indx, norm_indx) {
  inf = FALSE
  if (norm_indx == 'inf') {
    d <- data.frame(
      x1=abs((tr$trainPoints_x1 - ts$testPoints_x1[x_indx])),
      x2=abs(tr$trainPoints_x2 - ts$testPoints_x2[x_indx]))
    d <- pmax(d$x1, d$x2)
  } else {
    d <- (abs((tr$trainPoints_x1 - ts$testPoints_x1[x_indx])^norm_indx) + 
      (abs(tr$trainPoints_x2 - ts$testPoints_x2[x_indx])^norm_indx))^1/norm_indx
  }
  d <- data.frame(distance = d, weight=1/(d^2), label=tr$trainLabel)
  d <- d[order(d$distance),]

  return(head(d,3))

}
```

# KNN

With the weighted distance out of the way, we can use the function with the unique parameters. Again, we must use a condition for inifinity.

```{R}
knn <- function(x_indx, norm_indx) {
  n <- weighted_distance(x_indx, norm_indx)
  class <- which.max(
    array(data = c(sum(n[n$label == -1,]$weight), 
                   sum(n[n$label == 1,]$weight))
          )/sum(n$weight))
  if(sum(n[n$label == -1,]$weight)/sum(n$weight) > sum(n[n$label == 1,]$weight)/sum(n$weight)) {
    return(-1)
  } else {
    return(1)
  }
}
```

## $L_2$ norm 

```{R}
l2 <- lapply(1:40, knn, 2)
sum(l2 == ts$testLabel)/40
```

## $L_1$ norm 
```{R}
l1 <- lapply(1:40, knn, 1)
sum(l1 == ts$testLabel)/40
```

## $L_{\infty}$ norm
```{R}
linf <- lapply(1:40, knn, 'inf')
sum(linf == ts$testLabel)/40
```
# Accuracy

```{R}
df <- data.frame(
  l=1:3,
  accuracy=c(sum(l2 == ts$testLabel)/40, sum(l1 == ts$testLabel)/40, sum(linf == ts$testLabel)/40))
df
```
```{R}
x_lab <- c(TeX('$L_{2}$'), 'green', TeX('$L_{\\infty}$'))
  ggplot(df, aes(x=l, y=accuracy)) +
  geom_bar(stat="identity", fill="#377BB5") +
  geom_text(aes(label=unlist(df$accuracy)), vjust=1.5, color="white", size=5) +
  labs(title= "Accuracy", x = "Laplacian", y = "Accuracy") +
  theme(plot.title= element_text(size=rel(1.8), color="#377BB5"),
        axis.title.x= element_text(size=rel(1.2)),
        axis.title.y= element_text(size=rel(1.2))) +
  scale_x_discrete(limits = 1:3, labels= c(parse(text=TeX('$L_{2}$')),parse(text=TeX('$L_{1}$')),parse(text=TeX('$L_{\\infty}$'))))
  
```

```{R}+ 
  geom_text(aes(label=round(unlist(num2_df[772,2:6]),3)), vjust=1.5, color="white", size=rel(5)) +
  labs(title= "Likelihood of Pixel 772",
       x = "k",
       y = "Likelihood") +
  theme(plot.title= element_text(size=rel(1.8), color="#377BB5"),
        axis.title.x= element_text(size=rel(1.2)),
        axis.title.y= element_text(size=rel(1.2)))
```
